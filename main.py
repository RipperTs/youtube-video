from flask import Flask, render_template, request, jsonify, Response, send_file
from services.youtube_service import YouTubeService
from services.gemini_service import GeminiService
from services.stock_service import StockService
from services.report_service import ReportService
from services.cache_service import CacheService
from services.chart_service import ChartService
from config.settings import Config
import os
import json
import time
import re

app = Flask(__name__, 
           template_folder='web/templates',
           static_folder='web/static')

app.config.from_object(Config)


youtube_service = YouTubeService()
gemini_service = GeminiService()
stock_service = StockService()
report_service = ReportService()
cache_service = CacheService()
chart_service = ChartService()

@app.route('/')
def index():
    """é¦–é¡µ"""
    return render_template('index.html')

@app.route('/analyze', methods=['GET', 'POST'])
def analyze():
    """å•è§†é¢‘åˆ†æ"""
    if request.method == 'POST':
        return analyze_stream()
    
    return render_template('analyze.html')

@app.route('/analyze-stream', methods=['POST'])
def analyze_stream():
    """æµå¼åˆ†æè§†é¢‘"""
    # åœ¨è¯·æ±‚ä¸Šä¸‹æ–‡ä¸­è·å–æ•°æ®
    data = request.get_json()
    video_url = data.get('video_url')
    analysis_type = data.get('analysis_type', 'content_only')
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    stock_symbol = data.get('stock_symbol', 'AAPL') if analysis_type == 'manual_stock' else None
    
    def generate_analysis():
        try:
            # å‘é€åˆå§‹çŠ¶æ€
            yield f"data: {json.dumps({'type': 'status', 'message': 'å¼€å§‹åˆ†æ...', 'progress': 0})}\n\n"
            time.sleep(0.1)
            
            # åˆ›å»ºæ—¥å¿—å›è°ƒå‡½æ•°
            def log_callback(message, log_type, streaming_text=None):
                log_data = {
                    'type': 'log',
                    'message': message,
                    'log_type': log_type,
                    'timestamp': int(time.time() * 1000)
                }
                if streaming_text:
                    log_data['streaming_text'] = streaming_text
                return f"data: {json.dumps(log_data)}\n\n"
            
            # æ ¹æ®åˆ†æç±»å‹æ‰§è¡Œä¸åŒçš„é€»è¾‘
            if analysis_type == 'content_only':
                yield f"data: {json.dumps({'type': 'status', 'message': 'ä»…åˆ†æè§†é¢‘å†…å®¹å’ŒæŠ•èµ„é€»è¾‘', 'progress': 10})}\n\n"
                
                # æµå¼åˆ†æè§†é¢‘
                for log_output in _analyze_content_only_stream(video_url, log_callback):
                    yield log_output
                    
            elif analysis_type == 'stock_extraction':
                yield f"data: {json.dumps({'type': 'status', 'message': 'æå–è‚¡ç¥¨å¹¶åˆ†ææ•°æ®', 'progress': 10})}\n\n"
                
                # æµå¼è‚¡ç¥¨æå–åˆ†æ
                for log_output in _analyze_stock_extraction_stream(video_url, start_date, end_date, log_callback):
                    yield log_output
                    
            else:  # manual_stock
                yield f"data: {json.dumps({'type': 'status', 'message': 'æ‰‹åŠ¨æŒ‡å®šè‚¡ç¥¨åˆ†æ', 'progress': 10})}\n\n"
                
                for log_output in _analyze_manual_stock_stream(video_url, stock_symbol, start_date, end_date, log_callback):
                    yield log_output
            
        except Exception as e:
            error_data = {
                'type': 'error',
                'message': str(e),
                'timestamp': int(time.time() * 1000)
            }
            yield f"data: {json.dumps(error_data)}\n\n"
    
    return Response(generate_analysis(), mimetype='text/plain')

def _analyze_content_only_stream(video_url, log_callback):
    """æµå¼åˆ†æçº¯å†…å®¹"""
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_result = cache_service.get_cached_analysis_result(video_url)
        if cache_result['found']:
            yield log_callback("å‘ç°ç¼“å­˜ç»“æœï¼Œç›´æ¥è¿”å›", "info")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»ç¼“å­˜è·å–ç»“æœ', 'progress': 100})}\n\n"
            
            # ä»ç¼“å­˜è·å–å®Œæ•´çš„åˆ†æç»“æœ
            cached_analysis = cache_result['analysis_result']
            cached_analysis['from_cache'] = True
            
            # é‡è¦ï¼šè®¾ç½®cache_key
            cache_key = cache_service._generate_cache_key(video_url)
            cached_analysis['cache_key'] = cache_key
            print(f"ä»ç¼“å­˜è¿”å›ç»“æœï¼Œè®¾ç½®cache_key: {cache_key}")
            
            yield f"data: {json.dumps(cached_analysis)}\n\n"
            return
        
        # åˆ†æè§†é¢‘å†…å®¹
        yield log_callback("å¼€å§‹åˆ†æè§†é¢‘å†…å®¹...", "step")
        
        # ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†åˆ†æï¼ˆå¸¦æ—¥å¿—ï¼‰
        analysis_generator = gemini_service.analyze_video_with_logging(video_url, log_callback=log_callback)
        video_analysis = None
        
        for result in analysis_generator:
            if isinstance(result, str):  # æ—¥å¿—è¾“å‡º
                yield result
            else:  # æœ€ç»ˆç»“æœ
                video_analysis = result
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...', 'progress': 80})}\n\n"
        
        # ç”ŸæˆæŠ¥å‘Š
        yield log_callback("ç”Ÿæˆå†…å®¹åˆ†ææŠ¥å‘Š...", "info")
        report = report_service.generate_content_only_report(video_analysis)
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'åˆ†æå®Œæˆ!', 'progress': 100})}\n\n"
        
        # å‘é€æœ€ç»ˆç»“æœ
        result = {
            'type': 'result',
            'success': True,
            'analysis_type': 'content_only',
            'report': report,
            'video_analysis': video_analysis,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        yield log_callback("ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜...", "info")
        cache_key = cache_service.save_analysis_result(video_url, result)
        result['cache_key'] = cache_key
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'content_only',
            'video_analysis': video_analysis
        }
        cache_service.save_download_report(cache_key, report, video_url, metadata)
        
        yield f"data: {json.dumps(result)}\n\n"
        
    except Exception as e:
        yield log_callback(f"åˆ†æå¤±è´¥: {str(e)}", "error")

def _analyze_stock_extraction_stream(video_url, start_date, end_date, log_callback):
    """æµå¼è‚¡ç¥¨æå–åˆ†æ"""
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_result = cache_service.get_cached_analysis_result(video_url)
        if cache_result['found']:
            yield log_callback("å‘ç°ç¼“å­˜ç»“æœï¼Œç›´æ¥è¿”å›", "info")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»ç¼“å­˜è·å–ç»“æœ', 'progress': 100})}\n\n"
            
            # ä»ç¼“å­˜è·å–å®Œæ•´çš„åˆ†æç»“æœ
            cached_analysis = cache_result['analysis_result']
            cached_analysis['from_cache'] = True
            
            # é‡è¦ï¼šè®¾ç½®cache_key
            cache_key = cache_service._generate_cache_key(video_url)
            cached_analysis['cache_key'] = cache_key
            print(f"ä»ç¼“å­˜è¿”å›ç»“æœï¼Œè®¾ç½®cache_key: {cache_key}")
            
            yield f"data: {json.dumps(cached_analysis)}\n\n"
            return
        
        # æå–è‚¡ç¥¨ä¿¡æ¯
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨æå–è‚¡ç¥¨ä¿¡æ¯...', 'progress': 20})}\n\n"
        
        # ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†è‚¡ç¥¨æå–ï¼ˆå¸¦æ—¥å¿—ï¼‰
        extraction_generator = gemini_service.extract_stocks_from_video_with_logging(video_url, log_callback=log_callback)
        stock_extraction = None
        
        for result in extraction_generator:
            if isinstance(result, str):  # æ—¥å¿—è¾“å‡º
                yield result
            else:  # æœ€ç»ˆç»“æœ
                stock_extraction = result
        
        if stock_extraction is None:
            yield log_callback("è‚¡ç¥¨æå–å¤±è´¥", "error")
            return
            
        extracted_stocks = stock_extraction.get('extracted_stocks', [])
        
        if not extracted_stocks:
            yield log_callback("è§†é¢‘ä¸­æœªæ£€æµ‹åˆ°æ˜ç¡®çš„è‚¡ç¥¨ä¿¡æ¯", "error")
            return
        
        yield f"data: {json.dumps({'type': 'status', 'message': f'æ‰¾åˆ° {len(extracted_stocks)} åªè‚¡ç¥¨', 'progress': 40})}\n\n"
        
        # åˆ†æè§†é¢‘å†…å®¹
        yield log_callback("åˆ†æè§†é¢‘å†…å®¹...", "step")
        
        analysis_generator = gemini_service.analyze_video_with_logging(video_url, log_callback=log_callback)
        video_analysis = None
        
        for result in analysis_generator:
            if isinstance(result, str):  # æ—¥å¿—è¾“å‡º
                yield result
            else:  # æœ€ç»ˆç»“æœ
                video_analysis = result
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'è·å–è‚¡ç¥¨æ•°æ®...', 'progress': 60})}\n\n"
        
        # è·å–è‚¡ç¥¨æ•°æ®
        stock_data_list = []
        for i, stock in enumerate(extracted_stocks):
            try:
                yield log_callback(f"è·å– {stock['symbol']} è‚¡ç¥¨æ•°æ®...", "info")
                stock_data = stock_service.get_stock_data_by_date_range(stock['symbol'], start_date, end_date)
                stock_data['name'] = stock.get('name', '')
                stock_data_list.append(stock_data)
                progress = 60 + (i + 1) * 10 / len(extracted_stocks)
                yield f"data: {json.dumps({'type': 'status', 'message': f'å·²è·å– {i+1}/{len(extracted_stocks)} è‚¡ç¥¨æ•°æ®', 'progress': progress})}\n\n"
            except Exception as e:
                yield log_callback(f"è·å–è‚¡ç¥¨ {stock['symbol']} æ•°æ®å¤±è´¥: {str(e)}", "warning")
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...', 'progress': 80})}\n\n"
        
        # ç”ŸæˆæŠ¥å‘Š
        report = report_service.generate_stock_extraction_report(
            video_analysis, stock_data_list, extracted_stocks
        )
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'åˆ†æå®Œæˆ!', 'progress': 100})}\n\n"
        
        # å‘é€æœ€ç»ˆç»“æœ
        result = {
            'type': 'result',
            'success': True,
            'analysis_type': 'stock_extraction',
            'report': report,
            'video_analysis': video_analysis,
            'extracted_stocks': extracted_stocks,
            'stock_data': stock_data_list,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        yield log_callback("ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜...", "info")
        cache_key = cache_service.save_analysis_result(video_url, result)
        result['cache_key'] = cache_key
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'stock_extraction',
            'video_analysis': video_analysis,
            'extracted_stocks': extracted_stocks,
            'stock_data': stock_data_list
        }
        cache_service.save_download_report(cache_key, report, video_url, metadata)
        
        yield f"data: {json.dumps(result)}\n\n"
        
    except Exception as e:
        yield log_callback(f"åˆ†æå¤±è´¥: {str(e)}", "error")

def _analyze_manual_stock_stream(video_url, stock_symbol, start_date, end_date, log_callback):
    """æµå¼æ‰‹åŠ¨è‚¡ç¥¨åˆ†æ"""
    try:
        # æ£€æŸ¥ç¼“å­˜ï¼ˆç»“åˆè§†é¢‘URLå’Œè‚¡ç¥¨ä»£ç ï¼‰
        cache_urls = f"{video_url}|{stock_symbol}|{start_date}|{end_date}"
        cache_result = cache_service.get_cached_analysis_result(cache_urls)
        if cache_result['found']:
            yield log_callback("å‘ç°ç¼“å­˜ç»“æœï¼Œç›´æ¥è¿”å›", "info")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»ç¼“å­˜è·å–ç»“æœ', 'progress': 100})}\n\n"
            
            # ä»ç¼“å­˜è·å–å®Œæ•´çš„åˆ†æç»“æœ
            cached_analysis = cache_result['analysis_result']
            cached_analysis['from_cache'] = True
            
            # é‡è¦ï¼šè®¾ç½®cache_key
            cache_key = cache_service._generate_cache_key(cache_urls)
            cached_analysis['cache_key'] = cache_key
            print(f"ä»ç¼“å­˜è¿”å›ç»“æœï¼Œè®¾ç½®cache_key: {cache_key}")
            
            yield f"data: {json.dumps(cached_analysis)}\n\n"
            return
        
        # åˆ†æè§†é¢‘
        yield f"data: {json.dumps({'type': 'status', 'message': 'åˆ†æè§†é¢‘å†…å®¹...', 'progress': 30})}\n\n"
        
        analysis_generator = gemini_service.analyze_video_with_logging(video_url, log_callback=log_callback)
        video_analysis = None
        
        for result in analysis_generator:
            if isinstance(result, str):  # æ—¥å¿—è¾“å‡º
                yield result
            else:  # æœ€ç»ˆç»“æœ
                video_analysis = result
        
        yield f"data: {json.dumps({'type': 'status', 'message': f'è·å– {stock_symbol} è‚¡ç¥¨æ•°æ®...', 'progress': 60})}\n\n"
        
        # è·å–è‚¡ç¥¨æ•°æ®
        yield log_callback(f"è·å– {stock_symbol} è‚¡ç¥¨æ•°æ®...", "info")
        stock_data = stock_service.get_stock_data_by_date_range(stock_symbol, start_date, end_date)
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'ç”ŸæˆæŠ•èµ„åˆ†ææŠ¥å‘Š...', 'progress': 80})}\n\n"
        
        # ç”ŸæˆæŠ¥å‘Š
        yield log_callback("ç”ŸæˆæŠ•èµ„åˆ†ææŠ¥å‘Š...", "info")
        report = report_service.generate_report(video_analysis, stock_data)
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'åˆ†æå®Œæˆ!', 'progress': 100})}\n\n"
        
        # å‘é€æœ€ç»ˆç»“æœ
        result = {
            'type': 'result',
            'success': True,
            'analysis_type': 'manual_stock',
            'report': report,
            'video_analysis': video_analysis,
            'stock_data': stock_data,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        yield log_callback("ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜...", "info")
        cache_key = cache_service.save_analysis_result(cache_urls, result)
        result['cache_key'] = cache_key
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'manual_stock',
            'video_analysis': video_analysis,
            'stock_data': stock_data,
            'stock_symbol': stock_symbol,
            'start_date': start_date,
            'end_date': end_date
        }
        cache_service.save_download_report(cache_key, report, cache_urls, metadata)
        
        yield f"data: {json.dumps(result)}\n\n"
        
    except Exception as e:
        yield log_callback(f"åˆ†æå¤±è´¥: {str(e)}", "error")

@app.route('/batch-analyze', methods=['GET', 'POST'])
def batch_analyze():
    """æ‰¹é‡è§†é¢‘åˆ†æ"""
    if request.method == 'POST':
        try:
            data = request.json
            if not data:
                return jsonify({
                    'success': False,
                    'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
                }), 400
            
            channel_id = data.get('channel_id')
            video_count = min(data.get('video_count', 5), 10)  # é™åˆ¶æœ€å¤š10ä¸ªè§†é¢‘
            
            # è·å–é¢‘é“è§†é¢‘
            channel_result = youtube_service.get_channel_videos(channel_id, video_count)
            videos = channel_result['videos']
            
            # é™åˆ¶è§†é¢‘æ•°é‡ä¸º10ä¸ª
            videos = videos[:10]
            
            # æå–è§†é¢‘URLåˆ—è¡¨
            video_urls = [video.get('url') for video in videos if video.get('url')]
            
            if not video_urls:
                raise Exception("æœªè·å–åˆ°æœ‰æ•ˆçš„è§†é¢‘URL")
            
            # æ£€æŸ¥ç¼“å­˜
            cache_result = cache_service.get_cached_analysis_result(video_urls)
            if cache_result['found']:
                cached_result = cache_result['analysis_result']
                cached_result['from_cache'] = True
                return jsonify(cached_result)
            
            # ä½¿ç”¨Geminiæ‰¹é‡åˆ†æè§†é¢‘
            batch_analysis_generator = gemini_service.analyze_batch_videos(video_urls)
            batch_analysis = None
            
            for result in batch_analysis_generator:
                if not isinstance(result, str):  # æœ€ç»ˆç»“æœ
                    batch_analysis = result
                    break
            
            if not batch_analysis:
                raise Exception("æ‰¹é‡åˆ†æå¤±è´¥")
            
            # ç”Ÿæˆæ‰¹é‡å†…å®¹åˆ†ææŠ¥å‘Š
            report = report_service.generate_batch_content_report(batch_analysis)
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                'success': True,
                'report': report,
                'video_count': len(videos),
                'from_cache': False
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜
            cache_key = cache_service.save_analysis_result(video_urls, result)
            result['cache_key'] = cache_key
            
            # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
            metadata = {
                'analysis_type': 'batch_content',
                'batch_analysis': batch_analysis,
                'video_count': len(videos)
            }
            cache_service.save_download_report(cache_key, report, video_urls, metadata)
            
            return jsonify(result)
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500
    
    return render_template('batch_analyze.html')

@app.route('/api/channel-videos')
def get_channel_videos():
    """è·å–é¢‘é“è§†é¢‘API"""
    channel_id = request.args.get('channel_id')
    count = request.args.get('count', 20, type=int)
    next_token = request.args.get('next_token', '')
    
    try:
        result = youtube_service.get_channel_videos(channel_id, count, next_token)
        return jsonify({
            'success': True,
            'videos': result['videos'],
            'next_token': result['next_token'],
            'has_more': result['has_more'],
            'count': len(result['videos'])
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/batch-analyze-selected', methods=['POST'])
def batch_analyze_selected():
    """æ‰¹é‡åˆ†æé€‰å®šçš„è§†é¢‘"""
    try:
        data = request.json
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        selected_videos = data.get('selected_videos', [])
        
        if not selected_videos:
            return jsonify({
                'success': False,
                'error': 'è¯·é€‰æ‹©è¦åˆ†æçš„è§†é¢‘'
            }), 400
        
        if len(selected_videos) > 10:
            return jsonify({
                'success': False,
                'error': 'æœ€å¤šåªèƒ½é€‰æ‹©10ä¸ªè§†é¢‘è¿›è¡Œåˆ†æ'
            }), 400
        
        # æå–è§†é¢‘URLåˆ—è¡¨
        video_urls = [video.get('url') for video in selected_videos if video.get('url')]

        print(video_urls)
        
        if not video_urls:
            raise Exception("æœªè·å–åˆ°æœ‰æ•ˆçš„è§†é¢‘URL")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_result = cache_service.get_cached_analysis_result(video_urls)
        if cache_result['found']:
            cached_result = cache_result['analysis_result']
            cached_result['from_cache'] = True
            return jsonify(cached_result)
        
        # ä½¿ç”¨Geminiæ‰¹é‡åˆ†æè§†é¢‘
        batch_analysis_generator = gemini_service.analyze_batch_videos(video_urls)
        batch_analysis = None
        
        for result in batch_analysis_generator:
            if not isinstance(result, str):  # æœ€ç»ˆç»“æœ
                batch_analysis = result
                break
        
        if not batch_analysis:
            raise Exception("æ‰¹é‡åˆ†æå¤±è´¥")
        
        # ç”Ÿæˆæ‰¹é‡å†…å®¹åˆ†ææŠ¥å‘Š
        report = report_service.generate_batch_content_report(batch_analysis)
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'success': True,
            'report': report,
            'video_count': len(selected_videos),
            'selected_videos': selected_videos,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        cache_key = cache_service.save_analysis_result(video_urls, result)
        result['cache_key'] = cache_key
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'batch_selected',
            'batch_analysis': batch_analysis,
            'selected_videos': selected_videos,
            'video_count': len(selected_videos)
        }
        cache_service.save_download_report(cache_key, report, video_urls, metadata)
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/download-markdown/<cache_key>')
def download_markdown(cache_key):
    """ä¸‹è½½MarkdownæŠ¥å‘Š"""
    try:
        # è·å–Markdownæ–‡ä»¶è·¯å¾„
        markdown_file = cache_service.get_markdown_file_path(cache_key)
        
        if not os.path.exists(markdown_file):
            return jsonify({
                'success': False,
                'error': "ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨"
            }), 404
        
        # ç”Ÿæˆä¸‹è½½æ–‡ä»¶å
        filename = f"youtube_analysis_{cache_key[:8]}.md"
        
        return send_file(
            markdown_file,
            as_attachment=True,
            download_name=filename,
            mimetype='text/markdown'
        )
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f"Markdownä¸‹è½½å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/stock-data')
def get_stock_data():
    """è·å–è‚¡ç¥¨æ•°æ®API"""
    symbol = request.args.get('symbol', 'AAPL')
    days = request.args.get('days', 30, type=int)
    
    try:
        data = stock_service.get_stock_data(symbol, days)
        return jsonify({
            'success': True,
            'data': data
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/extract-stocks-chart', methods=['POST'])
def extract_stocks_chart():
    """æå–è‚¡ç¥¨ä¿¡æ¯å¹¶ç”Ÿæˆèµ°åŠ¿å›¾åˆ†æ"""
    try:
        data = request.get_json()
        cache_key = data.get('cache_key')
        date_range = data.get('date_range', 30)
        
        print(f"æ”¶åˆ°è‚¡ç¥¨æå–è¯·æ±‚ï¼Œcache_key: {cache_key}")
        
        if not cache_key:
            print("é”™è¯¯ï¼šç¼ºå°‘cache_keyå‚æ•°")
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘cache_keyå‚æ•°'
            }), 400
        
        # ä»ç¼“å­˜è·å–åˆ†æç»“æœ
        print(f"å°è¯•ä»ç¼“å­˜è·å–æ•°æ®ï¼Œcache_key: {cache_key}")
        cached_data = cache_service.get_analysis_result_by_key(cache_key)
        print(f"ç¼“å­˜æ•°æ®è·å–ç»“æœ: {cached_data is not None}")
        if not cached_data:
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°cache_key {cache_key} å¯¹åº”çš„åˆ†æç»“æœ")
            # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç¼“å­˜æ–‡ä»¶è¿›è¡Œè°ƒè¯•
            import os
            cache_dir = cache_service.analysis_cache_dir
            if os.path.exists(cache_dir):
                cache_files = os.listdir(cache_dir)
                print(f"å¯ç”¨çš„ç¼“å­˜æ–‡ä»¶: {cache_files}")
            else:
                print(f"ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_dir}")
            
            return jsonify({
                'success': False,
                'error': 'æœªæ‰¾åˆ°å¯¹åº”çš„åˆ†æç»“æœ'
            }), 404
        
        # æå–è‚¡ç¥¨ä¿¡æ¯
        extracted_stocks = extract_stocks_from_report(cached_data)
        
        if not extracted_stocks:
            return jsonify({
                'success': False,
                'error': 'æœªèƒ½ä»æŠ¥å‘Šä¸­æå–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä¿¡æ¯'
            }), 400
        
        # ä»ç¼“å­˜æ•°æ®ä¸­è·å–æ—¥æœŸèŒƒå›´ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        start_date = None
        end_date = None
        
        # å°è¯•ä»è‚¡ç¥¨æ•°æ®ä¸­è·å–æ—¥æœŸèŒƒå›´
        stock_data = cached_data.get('stock_data')
        if stock_data:
            if isinstance(stock_data, list) and len(stock_data) > 0:
                # å¤šè‚¡ç¥¨æ•°æ®
                first_stock = stock_data[0]
                start_date = first_stock.get('start_date')
                end_date = first_stock.get('end_date')
            elif isinstance(stock_data, dict):
                # å•è‚¡ç¥¨æ•°æ®
                start_date = stock_data.get('start_date')
                end_date = stock_data.get('end_date')
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤çš„30å¤©å‰åˆ°ä»Šå¤©
        if not start_date or not end_date:
            from datetime import datetime, timedelta
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            print(f"ğŸ”§ ä½¿ç”¨é»˜è®¤æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
        else:
            print(f"ğŸ“… ä»ç¼“å­˜è·å–çš„æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
        
        # ç”Ÿæˆè‚¡ç¥¨å›¾è¡¨
        stock_charts = []
        for stock in extracted_stocks:
            chart_result = chart_service.generate_stock_chart_by_date_range(stock['symbol'], start_date, end_date)
            # åŒ…å«æ‰€æœ‰ç»“æœï¼Œä¸è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥
            stock_charts.append(chart_result)
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            if chart_result.get('success'):
                print(f"âœ… æˆåŠŸç”Ÿæˆ {stock['symbol']} çš„å›¾è¡¨")
            else:
                print(f"âŒ ç”Ÿæˆ {stock['symbol']} å›¾è¡¨å¤±è´¥: {chart_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # ç”Ÿæˆå‡†ç¡®æ€§åˆ†æ
        accuracy_analysis = generate_accuracy_analysis(extracted_stocks, stock_charts, cached_data)
        
        # æ¸…ç†æ—§å›¾è¡¨
        chart_service.cleanup_old_charts()
        
        return jsonify({
            'success': True,
            'data': {
                'extracted_stocks': extracted_stocks,
                'stock_charts': stock_charts,
                'accuracy_analysis': accuracy_analysis
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'è‚¡ç¥¨æå–å’Œå›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}'
        }), 500

def extract_stocks_from_report(cached_data):
    """ä»åˆ†ææŠ¥å‘Šä¸­æå–è‚¡ç¥¨ä¿¡æ¯"""
    extracted_stocks = []
    
    try:
        # è·å–æŠ¥å‘Šå†…å®¹
        report = cached_data.get('report', {})
        video_analysis = cached_data.get('video_analysis', {})
        
        # å°è¯•ä»ä¸åŒå­—æ®µæå–è‚¡ç¥¨ä¿¡æ¯
        content_sources = []
        
        # ä»æŠ¥å‘Šä¸­æå–
        if report.get('raw_markdown_content'):
            content_sources.append(report['raw_markdown_content'])
        if report.get('executive_summary'):
            content_sources.append(report['executive_summary'])
        if report.get('investment_recommendation'):
            if isinstance(report['investment_recommendation'], dict):
                content_sources.append(report['investment_recommendation'].get('reasoning', ''))
            else:
                content_sources.append(str(report['investment_recommendation']))
        
        # ä»è§†é¢‘åˆ†æä¸­æå–
        if video_analysis.get('summary'):
            content_sources.append(video_analysis['summary'])
        if video_analysis.get('companies'):
            content_sources.extend(video_analysis['companies'])
        
        # åˆå¹¶æ‰€æœ‰å†…å®¹
        combined_content = ' '.join(content_sources)
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è‚¡ç¥¨ä»£ç 
        stock_pattern = r'\b([A-Z]{1,5})\b'
        potential_stocks = re.findall(stock_pattern, combined_content)
        
        # è¿‡æ»¤æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç 
        known_stocks = {
            'AAPL': {'name': 'Apple Inc.', 'confidence': 'high'},
            'GOOGL': {'name': 'Alphabet Inc.', 'confidence': 'high'},
            'GOOG': {'name': 'Alphabet Inc.', 'confidence': 'high'},
            'MSFT': {'name': 'Microsoft Corporation', 'confidence': 'high'},
            'AMZN': {'name': 'Amazon.com Inc.', 'confidence': 'high'},
            'TSLA': {'name': 'Tesla Inc.', 'confidence': 'high'},
            'META': {'name': 'Meta Platforms Inc.', 'confidence': 'high'},
            'NVDA': {'name': 'NVIDIA Corporation', 'confidence': 'high'},
            'NFLX': {'name': 'Netflix Inc.', 'confidence': 'high'},
            'CRM': {'name': 'Salesforce Inc.', 'confidence': 'high'},
            'ADBE': {'name': 'Adobe Inc.', 'confidence': 'high'},
            'ORCL': {'name': 'Oracle Corporation', 'confidence': 'high'},
            'IBM': {'name': 'IBM', 'confidence': 'high'},
            'INTC': {'name': 'Intel Corporation', 'confidence': 'high'},
            'AMD': {'name': 'Advanced Micro Devices', 'confidence': 'high'},
            'BABA': {'name': 'Alibaba Group', 'confidence': 'high'},
            'V': {'name': 'Visa Inc.', 'confidence': 'medium'},
            'MA': {'name': 'Mastercard Inc.', 'confidence': 'medium'},
        }
        
        # åˆ†æè‚¡ç¥¨å»ºè®®
        def extract_recommendation(content, symbol):
            content_lower = content.lower()
            if any(word in content_lower for word in ['ä¹°å…¥', 'å¢ä»“', 'çœ‹å¤š', 'buy', 'bullish']):
                return 'å»ºè®®ä¹°å…¥'
            elif any(word in content_lower for word in ['å–å‡º', 'å‡ä»“', 'çœ‹ç©º', 'sell', 'bearish']):
                return 'å»ºè®®å–å‡º'
            elif any(word in content_lower for word in ['æŒæœ‰', 'hold']):
                return 'å»ºè®®æŒæœ‰'
            else:
                return 'æ— æ˜ç¡®å»ºè®®'
        
        # å¤„ç†å‘ç°çš„è‚¡ç¥¨
        unique_stocks = list(set(potential_stocks))
        for symbol in unique_stocks:
            if symbol in known_stocks:
                extracted_stocks.append({
                    'symbol': symbol,
                    'name': known_stocks[symbol]['name'],
                    'confidence': known_stocks[symbol]['confidence'],
                    'recommendation': extract_recommendation(combined_content, symbol)
                })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨ï¼Œå°è¯•ä»å…¬å¸åç§°æ¨æ–­
        if not extracted_stocks:
            company_mappings = {
                'apple': 'AAPL',
                'google': 'GOOGL',
                'alphabet': 'GOOGL',
                'microsoft': 'MSFT',
                'amazon': 'AMZN',
                'tesla': 'TSLA',
                'meta': 'META',
                'facebook': 'META',
                'nvidia': 'NVDA',
                'netflix': 'NFLX'
            }
            
            content_lower = combined_content.lower()
            for company, symbol in company_mappings.items():
                if company in content_lower and symbol not in [s['symbol'] for s in extracted_stocks]:
                    extracted_stocks.append({
                        'symbol': symbol,
                        'name': known_stocks.get(symbol, {}).get('name', f'{symbol} Corporation'),
                        'confidence': 'medium',
                        'recommendation': extract_recommendation(combined_content, symbol)
                    })
        
        return extracted_stocks[:5]  # é™åˆ¶æœ€å¤š5åªè‚¡ç¥¨
        
    except Exception as e:
        print(f"è‚¡ç¥¨æå–å¤±è´¥: {e}")
        return []

def generate_accuracy_analysis(extracted_stocks, stock_charts, cached_data):
    """ç”Ÿæˆå‡†ç¡®æ€§åˆ†æ"""
    try:
        # ä»ä¸åŒå­—æ®µè·å–æŠ¥å‘Šæ‘˜è¦
        report = cached_data.get('report', {})
        video_analysis = cached_data.get('video_analysis', {})
        
        # å°è¯•è·å–æœ€ç›¸å…³çš„æŠ¥å‘Šå†…å®¹
        report_summary = ""
        
        # ä¼˜å…ˆçº§ï¼šexecutive_summary -> raw_markdown_content -> video_analysis.summary
        if report.get('executive_summary'):
            report_summary = report['executive_summary']
        elif report.get('raw_markdown_content'):
            # å¦‚æœæ˜¯Markdownå†…å®¹ï¼Œæå–å‰1000å­—ç¬¦ä½œä¸ºæ‘˜è¦
            raw_content = report['raw_markdown_content']
            report_summary = raw_content[:1000] + "..." if len(raw_content) > 1000 else raw_content
        elif video_analysis.get('summary'):
            report_summary = video_analysis['summary'][:1000] + "..." if len(video_analysis.get('summary', '')) > 1000 else video_analysis.get('summary', '')
        else:
            report_summary = "æ— å¯ç”¨çš„æŠ¥å‘Šæ‘˜è¦"
        
        # æ„å»ºåˆ†ææç¤ºè¯
        analysis_prompt = f"""
ä½œä¸ºä¸“ä¸šçš„æŠ•èµ„åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹YouTubeè§†é¢‘æŠ•èµ„å»ºè®®çš„å‡†ç¡®æ€§ï¼š

## æå–çš„è‚¡ç¥¨ä¿¡æ¯ï¼š
{json.dumps(extracted_stocks, ensure_ascii=False, indent=2)}

## å®é™…è‚¡ç¥¨è¡¨ç°ï¼š
{json.dumps([{
    'symbol': chart['symbol'],
    'current_price': chart.get('current_price'),
    'price_change': chart.get('price_change')
} for chart in stock_charts if chart.get('success')], ensure_ascii=False, indent=2)}

## åŸå§‹åˆ†ææŠ¥å‘Šæ‘˜è¦ï¼š
{report_summary}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œä¸“ä¸šåˆ†æï¼š
1. **è‚¡ç¥¨é€‰æ‹©åˆç†æ€§** - è¯„ä¼°é€‰è‚¡é€»è¾‘å’Œè´¨é‡
2. **æŠ•èµ„å»ºè®®å‡†ç¡®æ€§** - å¯¹æ¯”å»ºè®®ä¸å®é™…è¡¨ç°
3. **åˆ†æé€»è¾‘ä¸¥è°¨æ€§** - è¯„ä¼°æ¨ç†è¿‡ç¨‹å’Œä¾æ®
4. **æ•´ä½“å‡†ç¡®æ€§è¯„åˆ†** - ç»™å‡º1-10åˆ†çš„ç»¼åˆè¯„åˆ†

è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„è¯„åˆ†ç†ç”±å’Œæ”¹è¿›å»ºè®®ã€‚
æ ¼å¼è¦æ±‚ï¼šå¼€å¤´å°±ç›´æ¥ç»™å‡ºè¯„åˆ†ï¼Œå¦‚"ç»¼åˆå‡†ç¡®æ€§è¯„åˆ†: 7.5/10"
        """
        
        print("å¼€å§‹è°ƒç”¨Geminiè¿›è¡Œå‡†ç¡®æ€§åˆ†æ...")
        print(f"ä½¿ç”¨çš„æŠ¥å‘Šæ‘˜è¦é•¿åº¦: {len(report_summary)}")
        
        # è°ƒç”¨Geminiè¿›è¡Œåˆ†æ
        gemini_result = gemini_service.generate_text(analysis_prompt)
        
        if gemini_result.get('success'):
            analysis_content = gemini_result.get('summary', '')
            
            # å°è¯•ä»ç»“æœä¸­æå–è¯„åˆ†
            score_match = re.search(r'(\d+(?:\.\d+)?)/10', analysis_content)
            overall_score = f"{score_match.group(1)}/10" if score_match else "7.0/10"
            
            # æå–å…³é”®å‘ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            key_findings = []
            if 'è‚¡ç¥¨é€‰æ‹©' in analysis_content:
                key_findings.append('å·²åˆ†æè‚¡ç¥¨é€‰æ‹©åˆç†æ€§')
            if 'æŠ•èµ„å»ºè®®' in analysis_content:
                key_findings.append('å·²è¯„ä¼°æŠ•èµ„å»ºè®®å‡†ç¡®æ€§')
            if 'åˆ†æé€»è¾‘' in analysis_content:
                key_findings.append('å·²å®¡æŸ¥åˆ†æé€»è¾‘ä¸¥è°¨æ€§')
            
            return {
                'overall_score': overall_score,
                'analysis_summary': analysis_content,
                'key_findings': key_findings if key_findings else ['ç»¼åˆåˆ†æå·²å®Œæˆ'],
                'market_context': 'åŸºäºå½“å‰å¸‚åœºæ•°æ®è¿›è¡Œåˆ†æ'
            }
        else:
            print(f"Geminiåˆ†æå¤±è´¥: {gemini_result.get('error')}")
            return generate_fallback_accuracy_analysis(extracted_stocks, stock_charts)
        
    except Exception as e:
        print(f"å‡†ç¡®æ€§åˆ†æå¤±è´¥: {e}")
        return generate_fallback_accuracy_analysis(extracted_stocks, stock_charts)

def generate_fallback_accuracy_analysis(extracted_stocks, stock_charts):
    """ç”Ÿæˆå¤‡ç”¨çš„å‡†ç¡®æ€§åˆ†æ"""
    try:
        total_stocks = len(extracted_stocks)
        positive_performance = sum(1 for chart in stock_charts 
                                 if chart.get('success') and chart.get('price_change', 0) > 0)
        
        # åŸºäºè‚¡ç¥¨è¡¨ç°è®¡ç®—ç®€å•è¯„åˆ†
        if total_stocks > 0:
            performance_ratio = positive_performance / total_stocks
            base_score = 5.0 + (performance_ratio * 3.0)  # 5-8åˆ†åŒºé—´
        else:
            base_score = 6.0
            
        return {
            'overall_score': f"{base_score:.1f}/10",
            'analysis_summary': f"""
åŸºäºæ•°æ®åˆ†æçš„å‡†ç¡®æ€§è¯„ä¼°ï¼š

**è‚¡ç¥¨é€‰æ‹©åˆ†æï¼š**
- å…±æå–åˆ° {total_stocks} åªè‚¡ç¥¨è¿›è¡Œåˆ†æ
- å…¶ä¸­ {positive_performance} åªè‚¡ç¥¨è¡¨ç°ä¸ºæ­£æ”¶ç›Š

**æŠ•èµ„å»ºè®®å‡†ç¡®æ€§ï¼š**
- æ­£æ”¶ç›Šæ¯”ä¾‹: {positive_performance}/{total_stocks} ({performance_ratio*100:.1f}%)
- æ•´ä½“æŠ•èµ„å»ºè®®{('è¾ƒä¸ºå‡†ç¡®' if performance_ratio > 0.6 else 'æœ‰å¾…æ”¹è¿›')}

**ç»¼åˆè¯„ä¼°ï¼š**
æŠ•èµ„å»ºè®®åœ¨å½“å‰å¸‚åœºç¯å¢ƒä¸‹è¡¨ç°{'è‰¯å¥½' if performance_ratio > 0.5 else 'ä¸€èˆ¬'}ï¼Œ
å»ºè®®æŠ•èµ„è€…ç»“åˆä¸ªäººé£é™©æ‰¿å—èƒ½åŠ›å’Œå¸‚åœºç¯å¢ƒåšå‡ºå†³ç­–ã€‚
            """.strip(),
            'key_findings': [
                f'åˆ†æäº†{total_stocks}åªè‚¡ç¥¨çš„è¡¨ç°',
                f'{positive_performance}åªè‚¡ç¥¨è·å¾—æ­£æ”¶ç›Š',
                'æä¾›äº†åŸºäºæ•°æ®çš„å®¢è§‚è¯„ä¼°'
            ],
            'market_context': 'åŸºäºå®é™…è‚¡ç¥¨ä»·æ ¼æ•°æ®è¿›è¡Œåˆ†æ'
        }
        
    except Exception as e:
        return {
            'overall_score': 'N/A',
            'analysis_summary': f'æ— æ³•ç”Ÿæˆå‡†ç¡®æ€§åˆ†æ: {str(e)}',
            'key_findings': ['åˆ†æç”Ÿæˆå¤±è´¥'],
            'market_context': 'æ•°æ®ä¸è¶³'
        }

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=15000)