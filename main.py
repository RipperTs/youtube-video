from flask import Flask, render_template, request, jsonify, Response, send_file
from services.youtube_service import YouTubeService
from services.gemini_service import GeminiService
from services.stock_service import StockService
from services.report_service import ReportService
from services.cache_service import CacheService
from services.record_service import RecordService
from services.chart_service import ChartService
from config.settings import Config
from utils.time_utils import utc_str_to_bj
import os
import json
import time
import re

app = Flask(__name__, 
           template_folder='web/templates',
           static_folder='web/static')

app.config.from_object(Config)


youtube_service = YouTubeService()
gemini_service = GeminiService()
stock_service = StockService()
report_service = ReportService()
cache_service = CacheService()
chart_service = ChartService()
record_service = RecordService()

@app.route('/')
def index():
    """é¦–é¡µ"""
    return render_template('index.html')

@app.route('/analyze', methods=['GET', 'POST'])
def analyze():
    """å•è§†é¢‘åˆ†æ"""
    if request.method == 'POST':
        return analyze_stream()
    
    return render_template('analyze.html')

@app.route('/analyze-stream', methods=['POST'])
def analyze_stream():
    """æµå¼åˆ†æè§†é¢‘"""
    # åœ¨è¯·æ±‚ä¸Šä¸‹æ–‡ä¸­è·å–æ•°æ®
    data = request.get_json()
    video_url = data.get('video_url')
    analysis_type = data.get('analysis_type', 'content_only')
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    stock_symbol = data.get('stock_symbol', 'AAPL') if analysis_type == 'manual_stock' else None
    report_language = data.get('report_language', 'en')
    
    def generate_analysis():
        try:
            # å‘é€åˆå§‹çŠ¶æ€
            yield f"data: {json.dumps({'type': 'status', 'message': 'å¼€å§‹åˆ†æ...', 'progress': 0})}\n\n"
            time.sleep(0.1)
            
            # åˆ›å»ºæ—¥å¿—å›è°ƒå‡½æ•°
            def log_callback(message, log_type, streaming_text=None):
                log_data = {
                    'type': 'log',
                    'message': message,
                    'log_type': log_type,
                    'timestamp': int(time.time() * 1000)
                }
                if streaming_text:
                    log_data['streaming_text'] = streaming_text
                return f"data: {json.dumps(log_data)}\n\n"
            
            # æ ¹æ®åˆ†æç±»å‹æ‰§è¡Œä¸åŒçš„é€»è¾‘
            if analysis_type == 'content_only':
                yield f"data: {json.dumps({'type': 'status', 'message': 'ä»…åˆ†æè§†é¢‘å†…å®¹å’ŒæŠ•èµ„é€»è¾‘', 'progress': 10})}\n\n"
                
                # æµå¼åˆ†æè§†é¢‘
                for log_output in _analyze_content_only_stream(video_url, start_date, end_date, log_callback, report_language):
                    yield log_output
                    
            elif analysis_type == 'stock_extraction':
                yield f"data: {json.dumps({'type': 'status', 'message': 'æå–è‚¡ç¥¨å¹¶åˆ†ææ•°æ®', 'progress': 10})}\n\n"
                
                # æµå¼è‚¡ç¥¨æå–åˆ†æ
                for log_output in _analyze_stock_extraction_stream(video_url, start_date, end_date, log_callback, report_language):
                    yield log_output
                    
            else:  # manual_stock
                yield f"data: {json.dumps({'type': 'status', 'message': 'æ‰‹åŠ¨æŒ‡å®šè‚¡ç¥¨åˆ†æ', 'progress': 10})}\n\n"
                
                for log_output in _analyze_manual_stock_stream(video_url, stock_symbol, start_date, end_date, log_callback, report_language):
                    yield log_output
            
        except Exception as e:
            error_data = {
                'type': 'error',
                'message': str(e),
                'timestamp': int(time.time() * 1000)
            }
            yield f"data: {json.dumps(error_data)}\n\n"
    
    return Response(generate_analysis(), mimetype='text/plain')

def _analyze_content_only_stream(video_url,start_date, end_date,  log_callback, report_language='en'):
    """æµå¼åˆ†æçº¯å†…å®¹"""
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_result = cache_service.get_cached_analysis_result(video_url)
        if cache_result['found']:
            yield log_callback("å‘ç°ç¼“å­˜ç»“æœï¼Œç›´æ¥è¿”å›", "info")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»ç¼“å­˜è·å–ç»“æœ', 'progress': 100})}\n\n"
            
            # ä»ç¼“å­˜è·å–å®Œæ•´çš„åˆ†æç»“æœ
            cached_analysis = cache_result['analysis_result']
            cached_analysis['from_cache'] = True
            
            # é‡è¦ï¼šè®¾ç½®cache_key
            cache_key = cache_service._generate_cache_key(video_url)
            cached_analysis['cache_key'] = cache_key
            print(f"ä»ç¼“å­˜è¿”å›ç»“æœï¼Œè®¾ç½®cache_key: {cache_key}")
            
            yield f"data: {json.dumps(cached_analysis)}\n\n"
            return
        
        # åˆ†æè§†é¢‘å†…å®¹
        yield log_callback("å¼€å§‹åˆ†æè§†é¢‘å†…å®¹...", "step")
        
        # ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†åˆ†æï¼ˆå¸¦æ—¥å¿—ï¼‰
        analysis_generator = gemini_service.analyze_video_with_logging(video_url, log_callback=log_callback, language=report_language)
        video_analysis = None
        
        for result in analysis_generator:
            if isinstance(result, str):  # æ—¥å¿—è¾“å‡º
                yield result
            else:  # æœ€ç»ˆç»“æœ
                video_analysis = result
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...', 'progress': 80})}\n\n"
        
        # ç”ŸæˆæŠ¥å‘Š
        yield log_callback("ç”Ÿæˆå†…å®¹åˆ†ææŠ¥å‘Š...", "info")
        report = report_service.generate_content_only_report(video_analysis, language=report_language)
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'åˆ†æå®Œæˆ!', 'progress': 100})}\n\n"
        
        # å‘é€æœ€ç»ˆç»“æœ
        result = {
            'type': 'result',
            'success': True,
            'analysis_type': 'content_only',
            'report': report,
            'video_analysis': video_analysis,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        yield log_callback("ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜...", "info")
        cache_key = cache_service.save_analysis_result(video_url, result)
        result['cache_key'] = cache_key
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'content_only',
            'video_analysis': video_analysis
        }
        cache_service.save_download_report(cache_key, report, video_url, metadata)
        
        # å†™å…¥åˆ†æè®°å½•ï¼ˆå•è§†é¢‘åˆ†æï¼‰
        try:
            record_service.add_record(
                video_url=video_url,
                channel_name=None,
                cache_key=cache_key,
                analysis_type='å•è§†é¢‘åˆ†æ',
                start_date=start_date,
                end_date=end_date,
                report_language=report_language,
            )
        except Exception as _:
            # å¿½ç•¥è®°å½•å¤±è´¥ï¼Œé¿å…å½±å“ä¸»æµç¨‹
            pass
        
        yield f"data: {json.dumps(result)}\n\n"
        
    except Exception as e:
        yield log_callback(f"åˆ†æå¤±è´¥: {str(e)}", "error")

def _analyze_stock_extraction_stream(video_url, start_date, end_date, log_callback, report_language='en'):
    """æµå¼è‚¡ç¥¨æå–åˆ†æ"""
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_result = cache_service.get_cached_analysis_result(video_url)
        if cache_result['found']:
            yield log_callback("å‘ç°ç¼“å­˜ç»“æœï¼Œç›´æ¥è¿”å›", "info")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»ç¼“å­˜è·å–ç»“æœ', 'progress': 100})}\n\n"
            
            # ä»ç¼“å­˜è·å–å®Œæ•´çš„åˆ†æç»“æœ
            cached_analysis = cache_result['analysis_result']
            cached_analysis['from_cache'] = True
            
            # é‡è¦ï¼šè®¾ç½®cache_key
            cache_key = cache_service._generate_cache_key(video_url)
            cached_analysis['cache_key'] = cache_key
            print(f"ä»ç¼“å­˜è¿”å›ç»“æœï¼Œè®¾ç½®cache_key: {cache_key}")
            
            yield f"data: {json.dumps(cached_analysis)}\n\n"
            return
        
        # æå–è‚¡ç¥¨ä¿¡æ¯
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨æå–è‚¡ç¥¨ä¿¡æ¯...', 'progress': 20})}\n\n"
        
        # ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†è‚¡ç¥¨æå–ï¼ˆå¸¦æ—¥å¿—ï¼‰
        extraction_generator = gemini_service.extract_stocks_from_video_with_logging(video_url, log_callback=log_callback)
        stock_extraction = None
        
        for result in extraction_generator:
            if isinstance(result, str):  # æ—¥å¿—è¾“å‡º
                yield result
            else:  # æœ€ç»ˆç»“æœ
                stock_extraction = result
        
        if stock_extraction is None:
            yield log_callback("è‚¡ç¥¨æå–å¤±è´¥", "error")
            return
            
        extracted_stocks = stock_extraction.get('extracted_stocks', [])
        
        if not extracted_stocks:
            yield log_callback("è§†é¢‘ä¸­æœªæ£€æµ‹åˆ°æ˜ç¡®çš„è‚¡ç¥¨ä¿¡æ¯", "error")
            return
        
        yield f"data: {json.dumps({'type': 'status', 'message': f'æ‰¾åˆ° {len(extracted_stocks)} åªè‚¡ç¥¨', 'progress': 40})}\n\n"
        
        # åˆ†æè§†é¢‘å†…å®¹
        yield log_callback("åˆ†æè§†é¢‘å†…å®¹...", "step")
        
        analysis_generator = gemini_service.analyze_video_with_logging(video_url, log_callback=log_callback, language=report_language)
        video_analysis = None
        
        for result in analysis_generator:
            if isinstance(result, str):  # æ—¥å¿—è¾“å‡º
                yield result
            else:  # æœ€ç»ˆç»“æœ
                video_analysis = result
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'è·å–è‚¡ç¥¨æ•°æ®...', 'progress': 60})}\n\n"
        
        # è·å–è‚¡ç¥¨æ•°æ®
        stock_data_list = []
        for i, stock in enumerate(extracted_stocks):
            try:
                yield log_callback(f"è·å– {stock['symbol']} è‚¡ç¥¨æ•°æ®...", "info")
                stock_data = stock_service.get_stock_data_by_date_range(stock['symbol'], start_date, end_date)
                stock_data['name'] = stock.get('name', '')
                stock_data_list.append(stock_data)
                progress = 60 + (i + 1) * 10 / len(extracted_stocks)
                yield f"data: {json.dumps({'type': 'status', 'message': f'å·²è·å– {i+1}/{len(extracted_stocks)} è‚¡ç¥¨æ•°æ®', 'progress': progress})}\n\n"
            except Exception as e:
                yield log_callback(f"è·å–è‚¡ç¥¨ {stock['symbol']} æ•°æ®å¤±è´¥: {str(e)}", "warning")
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...', 'progress': 80})}\n\n"
        
        # ç”ŸæˆæŠ¥å‘Š
        report = report_service.generate_stock_extraction_report(
            video_analysis, stock_data_list, extracted_stocks
        )
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'åˆ†æå®Œæˆ!', 'progress': 100})}\n\n"
        
        # å‘é€æœ€ç»ˆç»“æœ
        result = {
            'type': 'result',
            'success': True,
            'analysis_type': 'stock_extraction',
            'report': report,
            'video_analysis': video_analysis,
            'extracted_stocks': extracted_stocks,
            'stock_data': stock_data_list,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        yield log_callback("ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜...", "info")
        cache_key = cache_service.save_analysis_result(video_url, result)
        result['cache_key'] = cache_key
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'stock_extraction',
            'video_analysis': video_analysis,
            'extracted_stocks': extracted_stocks,
            'stock_data': stock_data_list
        }
        cache_service.save_download_report(cache_key, report, video_url, metadata)
        
        # å†™å…¥åˆ†æè®°å½•ï¼ˆå•è§†é¢‘åˆ†æ-è‚¡ç¥¨æå–ï¼‰
        try:
            record_service.add_record(
                video_url=video_url,
                channel_name=None,
                cache_key=cache_key,
                analysis_type='å•è§†é¢‘åˆ†æ',
                start_date=start_date,
                end_date=end_date,
                report_language=report_language,
            )
        except Exception as _:
            pass

        yield f"data: {json.dumps(result)}\n\n"
        
    except Exception as e:
        yield log_callback(f"åˆ†æå¤±è´¥: {str(e)}", "error")

def _analyze_manual_stock_stream(video_url, stock_symbol, start_date, end_date, log_callback, report_language='en'):
    """æµå¼æ‰‹åŠ¨è‚¡ç¥¨åˆ†æ"""
    try:
        # æ£€æŸ¥ç¼“å­˜ï¼ˆç»“åˆè§†é¢‘URLå’Œè‚¡ç¥¨ä»£ç ï¼‰
        cache_urls = f"{video_url}|{stock_symbol}|{start_date}|{end_date}"
        cache_result = cache_service.get_cached_analysis_result(cache_urls)
        if cache_result['found']:
            yield log_callback("å‘ç°ç¼“å­˜ç»“æœï¼Œç›´æ¥è¿”å›", "info")
            yield f"data: {json.dumps({'type': 'status', 'message': 'ä»ç¼“å­˜è·å–ç»“æœ', 'progress': 100})}\n\n"
            
            # ä»ç¼“å­˜è·å–å®Œæ•´çš„åˆ†æç»“æœ
            cached_analysis = cache_result['analysis_result']
            cached_analysis['from_cache'] = True
            
            # é‡è¦ï¼šè®¾ç½®cache_key
            cache_key = cache_service._generate_cache_key(cache_urls)
            cached_analysis['cache_key'] = cache_key
            print(f"ä»ç¼“å­˜è¿”å›ç»“æœï¼Œè®¾ç½®cache_key: {cache_key}")
            
            yield f"data: {json.dumps(cached_analysis)}\n\n"
            return
        
        # åˆ†æè§†é¢‘
        yield f"data: {json.dumps({'type': 'status', 'message': 'åˆ†æè§†é¢‘å†…å®¹...', 'progress': 30})}\n\n"
        
        analysis_generator = gemini_service.analyze_video_with_logging(video_url, log_callback=log_callback, language=report_language)
        video_analysis = None
        
        for result in analysis_generator:
            if isinstance(result, str):  # æ—¥å¿—è¾“å‡º
                yield result
            else:  # æœ€ç»ˆç»“æœ
                video_analysis = result
        
        yield f"data: {json.dumps({'type': 'status', 'message': f'è·å– {stock_symbol} è‚¡ç¥¨æ•°æ®...', 'progress': 60})}\n\n"
        
        # è·å–è‚¡ç¥¨æ•°æ®
        yield log_callback(f"è·å– {stock_symbol} è‚¡ç¥¨æ•°æ®...", "info")
        stock_data = stock_service.get_stock_data_by_date_range(stock_symbol, start_date, end_date)
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'ç”ŸæˆæŠ•èµ„åˆ†ææŠ¥å‘Š...', 'progress': 80})}\n\n"
        
        # ç”ŸæˆæŠ¥å‘Š
        yield log_callback("ç”ŸæˆæŠ•èµ„åˆ†ææŠ¥å‘Š...", "info")
        report = report_service.generate_report(video_analysis, stock_data)
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'åˆ†æå®Œæˆ!', 'progress': 100})}\n\n"
        
        # å‘é€æœ€ç»ˆç»“æœ
        result = {
            'type': 'result',
            'success': True,
            'analysis_type': 'manual_stock',
            'report': report,
            'video_analysis': video_analysis,
            'stock_data': stock_data,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        yield log_callback("ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜...", "info")
        cache_key = cache_service.save_analysis_result(cache_urls, result)
        result['cache_key'] = cache_key
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'manual_stock',
            'video_analysis': video_analysis,
            'stock_data': stock_data,
            'stock_symbol': stock_symbol,
            'start_date': start_date,
            'end_date': end_date
        }
        cache_service.save_download_report(cache_key, report, cache_urls, metadata)
        
        # å†™å…¥åˆ†æè®°å½•ï¼ˆå•è§†é¢‘åˆ†æï¼‰
        try:
            record_service.add_record(
                video_url=video_url,
                channel_name=None,
                cache_key=cache_key,
                analysis_type='å•è§†é¢‘åˆ†æ',
                start_date=start_date,
                end_date=end_date,
                report_language=report_language,
            )
        except Exception as _:
            pass
        
        yield f"data: {json.dumps(result)}\n\n"
        
    except Exception as e:
        yield log_callback(f"åˆ†æå¤±è´¥: {str(e)}", "error")

@app.route('/batch-analyze', methods=['GET', 'POST'])
def batch_analyze():
    """æ‰¹é‡è§†é¢‘åˆ†æ"""
    if request.method == 'POST':
        try:
            data = request.json
            if not data:
                return jsonify({
                    'success': False,
                    'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
                }), 400
            
            channel_id = data.get('channel_id')
            video_count = min(data.get('video_count', 5), 10)  # é™åˆ¶æœ€å¤š10ä¸ªè§†é¢‘
            report_language = data.get('report_language', 'en')
            
            # è·å–é¢‘é“è§†é¢‘
            channel_result = youtube_service.get_channel_videos(channel_id, video_count)
            videos = channel_result['videos']
            
            # é™åˆ¶è§†é¢‘æ•°é‡ä¸º10ä¸ª
            videos = videos[:10]
            
            # æå–è§†é¢‘URLåˆ—è¡¨
            video_urls = [video.get('url') for video in videos if video.get('url')]
            
            if not video_urls:
                raise Exception("æœªè·å–åˆ°æœ‰æ•ˆçš„è§†é¢‘URL")
            
            # æ£€æŸ¥ç¼“å­˜
            cache_result = cache_service.get_cached_analysis_result(video_urls)
            if cache_result['found']:
                cached_result = cache_result['analysis_result']
                cached_result['from_cache'] = True
                return jsonify(cached_result)
            
            # ä½¿ç”¨Geminiæ‰¹é‡åˆ†æè§†é¢‘
            batch_analysis_generator = gemini_service.analyze_batch_videos(video_urls)
            batch_analysis = None
            
            for result in batch_analysis_generator:
                if not isinstance(result, str):  # æœ€ç»ˆç»“æœ
                    batch_analysis = result
                    break
            
            if not batch_analysis:
                raise Exception("æ‰¹é‡åˆ†æå¤±è´¥")
            
            # ç”Ÿæˆæ‰¹é‡å†…å®¹åˆ†ææŠ¥å‘Š
            report = report_service.generate_batch_content_report(batch_analysis, language=report_language)
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                'success': True,
                'report': report,
                'video_count': len(videos),
                'from_cache': False
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜
            cache_key = cache_service.save_analysis_result(video_urls, result)
            result['cache_key'] = cache_key
            
            # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
            metadata = {
                'analysis_type': 'batch_content',
                'batch_analysis': batch_analysis,
                'video_count': len(videos)
            }
            cache_service.save_download_report(cache_key, report, video_urls, metadata)
            
            # æ‰¹é‡åˆ†æä¸å†™å…¥è®°å½•ï¼ˆæŒ‰éœ€ä¿ç•™ç¼“å­˜ä¸æŠ¥å‘Šï¼‰
            
            return jsonify(result)
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500
    
    return render_template('batch_analyze.html')

@app.route('/api/channel-videos')
def get_channel_videos():
    """è·å–é¢‘é“è§†é¢‘API"""
    channel_id = request.args.get('channel_id')
    count = request.args.get('count', 20, type=int)
    next_token = request.args.get('next_token', '')
    
    try:
        result = youtube_service.get_channel_videos(channel_id, count, next_token)
        return jsonify({
            'success': True,
            'videos': result['videos'],
            'next_token': result['next_token'],
            'has_more': result['has_more'],
            'count': len(result['videos'])
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/analysis-history')
def analysis_history():
    """æŸ¥è¯¢åˆ†æå†å²ï¼ˆå€’åºï¼‰ã€‚æ”¯æŒå‚æ•°ï¼šlimitï¼ˆé»˜è®¤50ï¼‰ã€‚"""
    try:
        limit = request.args.get('limit', default=50, type=int)
        records = record_service.list_records(limit=limit)
        # å°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´
        for rec in records:
            if isinstance(rec, dict) and 'created_at' in rec:
                rec['created_at'] = utc_str_to_bj(rec.get('created_at'))
        return jsonify({
            'success': True,
            'data': records,
            'count': len(records)
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/batch-analyze-selected', methods=['POST'])
def batch_analyze_selected():
    """æ‰¹é‡åˆ†æé€‰å®šçš„è§†é¢‘"""
    try:
        data = request.json
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        selected_videos = data.get('selected_videos', [])
        report_language = data.get('report_language', 'en')
        
        if not selected_videos:
            return jsonify({
                'success': False,
                'error': 'è¯·é€‰æ‹©è¦åˆ†æçš„è§†é¢‘'
            }), 400
        
        if len(selected_videos) > 10:
            return jsonify({
                'success': False,
                'error': 'æœ€å¤šåªèƒ½é€‰æ‹©10ä¸ªè§†é¢‘è¿›è¡Œåˆ†æ'
            }), 400
        
        # æå–è§†é¢‘URLåˆ—è¡¨
        video_urls = [video.get('url') for video in selected_videos if video.get('url')]

        print(video_urls)
        
        if not video_urls:
            raise Exception("æœªè·å–åˆ°æœ‰æ•ˆçš„è§†é¢‘URL")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_result = cache_service.get_cached_analysis_result(video_urls)
        if cache_result['found']:
            cached_result = cache_result['analysis_result']
            cached_result['from_cache'] = True
            return jsonify(cached_result)
        
        # ä½¿ç”¨Geminiæ‰¹é‡åˆ†æè§†é¢‘
        batch_analysis_generator = gemini_service.analyze_batch_videos(video_urls, language=report_language)
        batch_analysis = None
        
        for result in batch_analysis_generator:
            if not isinstance(result, str):  # æœ€ç»ˆç»“æœ
                batch_analysis = result
                break
        
        if not batch_analysis:
            raise Exception("æ‰¹é‡åˆ†æå¤±è´¥")
        
        # ç”Ÿæˆæ‰¹é‡å†…å®¹åˆ†ææŠ¥å‘Š
        report = report_service.generate_batch_content_report(batch_analysis, language=report_language)
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'success': True,
            'report': report,
            'video_count': len(selected_videos),
            'selected_videos': selected_videos,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        cache_key = cache_service.save_analysis_result(video_urls, result)
        result['cache_key'] = cache_key
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'batch_selected',
            'batch_analysis': batch_analysis,
            'selected_videos': selected_videos,
            'video_count': len(selected_videos)
        }
        cache_service.save_download_report(cache_key, report, video_urls, metadata)
        
        # æ‰¹é‡åˆ†æï¼ˆé€‰å®šï¼‰ä¸å†™å…¥è®°å½•ï¼ˆæŒ‰éœ€ä¿ç•™ç¼“å­˜ä¸æŠ¥å‘Šï¼‰

        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/download-pdf/<cache_key>')
def download_pdf(cache_key):
    """ä¸‹è½½PDFæŠ¥å‘Š"""
    try:
        # ä»ç¼“å­˜è·å–åˆ†æç»“æœ
        cached_data = cache_service.get_analysis_result_by_key(cache_key)
        if not cached_data:
            return jsonify({
                'success': False,
                'error': "ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨"
            }), 404
        
        # è·å–è§†é¢‘URLåˆ—è¡¨
        video_urls = cache_service.get_video_urls_by_cache_key(cache_key)
        if not video_urls:
            # å¦‚æœæ— æ³•è·å–URLï¼Œä½¿ç”¨å ä½ç¬¦
            video_urls = ["è§†é¢‘URLè·å–å¤±è´¥"]
        
        # ç”ŸæˆPDFæ–‡ä»¶
        pdf_file = report_service.generate_pdf_report(cache_key, cached_data, video_urls)
        
        if not os.path.exists(pdf_file):
            return jsonify({
                'success': False,
                'error': "PDFç”Ÿæˆå¤±è´¥"
            }), 500
        
        # ç”Ÿæˆä¸‹è½½æ–‡ä»¶å
        filename = f"youtube_analysis_{cache_key[:8]}.pdf"
        
        return send_file(
            pdf_file,
            as_attachment=True,
            download_name=filename,
            mimetype='application/pdf'
        )
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f"PDFä¸‹è½½å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/stock-data')
def get_stock_data():
    """è·å–è‚¡ç¥¨æ•°æ®API"""
    symbol = request.args.get('symbol', 'AAPL')
    days = request.args.get('days', 30, type=int)
    
    try:
        data = stock_service.get_stock_data(symbol, days)
        return jsonify({
            'success': True,
            'data': data
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/extract-stocks-chart', methods=['POST'])
def extract_stocks_chart():
    """æå–è‚¡ç¥¨ä¿¡æ¯å¹¶ç”Ÿæˆèµ°åŠ¿å›¾åˆ†æ"""
    try:
        data = request.get_json()
        cache_key = data.get('cache_key')
        # æ–°å¢ï¼šä»è¯·æ±‚ä¸­è·å–æ—¥æœŸèŒƒå›´
        request_start_date = data.get('start_date')
        request_end_date = data.get('end_date')
        
        print(f"æ”¶åˆ°è‚¡ç¥¨æå–è¯·æ±‚ï¼Œcache_key: {cache_key}")
        print(f"è¯·æ±‚çš„æ—¥æœŸèŒƒå›´: {request_start_date} åˆ° {request_end_date}")
        
        if not cache_key:
            print("é”™è¯¯ï¼šç¼ºå°‘cache_keyå‚æ•°")
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘cache_keyå‚æ•°'
            }), 400
        
        # ä»ç¼“å­˜è·å–åˆ†æç»“æœ
        print(f"å°è¯•ä»ç¼“å­˜è·å–æ•°æ®ï¼Œcache_key: {cache_key}")
        cached_data = cache_service.get_analysis_result_by_key(cache_key)
        print(f"ç¼“å­˜æ•°æ®è·å–ç»“æœ: {cached_data is not None}")
        if not cached_data:
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°cache_key {cache_key} å¯¹åº”çš„åˆ†æç»“æœ")
            # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç¼“å­˜æ–‡ä»¶è¿›è¡Œè°ƒè¯•
            import os
            cache_dir = cache_service.analysis_cache_dir
            if os.path.exists(cache_dir):
                cache_files = os.listdir(cache_dir)
                print(f"å¯ç”¨çš„ç¼“å­˜æ–‡ä»¶: {cache_files}")
            else:
                print(f"ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_dir}")
            
            return jsonify({
                'success': False,
                'error': 'æœªæ‰¾åˆ°å¯¹åº”çš„åˆ†æç»“æœ'
            }), 404
        
        # æå–è‚¡ç¥¨ä¿¡æ¯
        extracted_stocks = extract_stocks_from_report(cached_data)
        
        if not extracted_stocks:
            return jsonify({
                'success': False,
                'error': 'æœªèƒ½ä»æŠ¥å‘Šä¸­æå–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä¿¡æ¯'
            }), 400
        
        # ç¡®å®šä½¿ç”¨çš„æ—¥æœŸèŒƒå›´ï¼šä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä¸­çš„æ—¥æœŸï¼Œç„¶åæ˜¯ç¼“å­˜ä¸­çš„æ—¥æœŸï¼Œæœ€åä½¿ç”¨é»˜è®¤å€¼
        start_date = request_start_date
        end_date = request_end_date
        
        # å¦‚æœè¯·æ±‚ä¸­æ²¡æœ‰æ—¥æœŸèŒƒå›´ï¼Œå°è¯•ä»ç¼“å­˜çš„è‚¡ç¥¨æ•°æ®ä¸­è·å–
        if not start_date or not end_date:
            stock_data = cached_data.get('stock_data')
            if stock_data:
                if isinstance(stock_data, list) and len(stock_data) > 0:
                    # å¤šè‚¡ç¥¨æ•°æ®
                    first_stock = stock_data[0]
                    start_date = first_stock.get('start_date')
                    end_date = first_stock.get('end_date')
                elif isinstance(stock_data, dict):
                    # å•è‚¡ç¥¨æ•°æ®
                    start_date = stock_data.get('start_date')
                    end_date = stock_data.get('end_date')
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤çš„30å¤©å‰åˆ°ä»Šå¤©
        if not start_date or not end_date:
            from datetime import datetime, timedelta
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            print(f"ğŸ”§ ä½¿ç”¨é»˜è®¤æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
        else:
            print(f"ğŸ“… ä½¿ç”¨çš„æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
            if request_start_date and request_end_date:
                print("   (æ¥æº: ç”¨æˆ·è¯·æ±‚)")
            else:
                print("   (æ¥æº: ç¼“å­˜æ•°æ®)")
        
        # ç”Ÿæˆè‚¡ç¥¨å›¾è¡¨
        stock_charts = []
        for stock in extracted_stocks:
            chart_result = chart_service.generate_stock_chart_by_date_range(stock['symbol'], start_date, end_date)
            # åŒ…å«æ‰€æœ‰ç»“æœï¼Œä¸è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥
            stock_charts.append(chart_result)
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            if chart_result.get('success'):
                print(f"âœ… æˆåŠŸç”Ÿæˆ {stock['symbol']} çš„å›¾è¡¨")
            else:
                print(f"âŒ ç”Ÿæˆ {stock['symbol']} å›¾è¡¨å¤±è´¥: {chart_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # ç”Ÿæˆå‡†ç¡®æ€§åˆ†æ
        accuracy_analysis = generate_accuracy_analysis(extracted_stocks, stock_charts, cached_data)
        
        # æ¸…ç†æ—§å›¾è¡¨
        chart_service.cleanup_old_charts()
        
        return jsonify({
            'success': True,
            'data': {
                'extracted_stocks': extracted_stocks,
                'stock_charts': stock_charts,
                'accuracy_analysis': accuracy_analysis
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'è‚¡ç¥¨æå–å’Œå›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}'
        }), 500

def extract_stocks_from_report(cached_data):
    """ä½¿ç”¨AIæ™ºèƒ½åˆ†æä»åˆ†ææŠ¥å‘Šä¸­æå–è‚¡ç¥¨ä¿¡æ¯"""
    try:
        print(f"ğŸ¤– å¼€å§‹ä½¿ç”¨AIæ™ºèƒ½æå–è‚¡ç¥¨ä¿¡æ¯")
        
        # è·å–æŠ¥å‘Šå†…å®¹
        report = cached_data.get('report', {})
        video_analysis = cached_data.get('video_analysis', {})
        
        # æ„å»ºå®Œæ•´çš„åˆ†æå†…å®¹
        analysis_content = ""
        
        if report.get('raw_markdown_content'):
            analysis_content = report['raw_markdown_content']
        elif video_analysis.get('summary'):
            analysis_content = video_analysis['summary']
        elif report.get('executive_summary'):
            analysis_content = report['executive_summary']
        else:
            print("âŒ æœªæ‰¾åˆ°å¯åˆ†æçš„å†…å®¹")
            return []
        
        print(f"ğŸ“Š å‡†å¤‡åˆ†æçš„å†…å®¹é•¿åº¦: {len(analysis_content)}")
        
        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¶…è¿‡APIé™åˆ¶
        if len(analysis_content) > 10000:
            analysis_content = analysis_content[:10000] + "..."
            print(f"ğŸ“Š å†…å®¹è¿‡é•¿ï¼Œæˆªå–å‰10000å­—ç¬¦")
        
        # ä½¿ç”¨Gemini AIåˆ†ææå–è‚¡ç¥¨ä¿¡æ¯
        extracted_stocks = analyze_stocks_with_ai(analysis_content)
        
        if extracted_stocks:
            print(f"âœ… AIæˆåŠŸæå–åˆ° {len(extracted_stocks)} åªè‚¡ç¥¨")
            for stock in extracted_stocks:
                print(f"  ğŸ“ˆ {stock['symbol']} - {stock['name']} ({stock['confidence']})")
        else:
            print("âŒ AIæœªèƒ½æå–åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä¿¡æ¯")
        
        return extracted_stocks
        
    except Exception as e:
        print(f"âŒ AIè‚¡ç¥¨æå–å¤±è´¥: {e}")
        return []


def analyze_stocks_with_ai(content):
    """ä½¿ç”¨AIåˆ†æå†…å®¹å¹¶æå–è‚¡ç¥¨ä¿¡æ¯"""
    try:
        from services.gemini_service import GeminiService
        
        gemini_service = GeminiService()
        
        # æ„å»ºAIåˆ†ææç¤ºè¯
        prompt = f"""
ä½œä¸ºä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·ä»”ç»†åˆ†æä»¥ä¸‹æŠ•èµ„æŠ¥å‘Šå†…å®¹ï¼Œæå–å…¶ä¸­æåˆ°çš„æ‰€æœ‰ç¾è‚¡è‚¡ç¥¨ä¿¡æ¯ã€‚

**åˆ†æå†…å®¹ï¼š**
{content}

**è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œåªè¿”å›JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š**

{{
    "stocks": [
        {{
            "symbol": "è‚¡ç¥¨ä»£ç (å¦‚AAPL)",
            "name": "å…¬å¸å…¨å(å¦‚Apple Inc.)",
            "confidence": "æå–ç½®ä¿¡åº¦(high/medium/low)",
            "recommendation": "æŠ•èµ„å»ºè®®(ä¹°å…¥/å–å‡º/æŒæœ‰/æ— æ˜ç¡®å»ºè®®)",
            "context": "åœ¨æŠ¥å‘Šä¸­çš„ç›¸å…³æè¿°(ä¸è¶…è¿‡100å­—)"
        }}
    ]
}}

**é‡è¦è¦æ±‚ï¼š**
1. åªæå–åœ¨ç¾å›½äº¤æ˜“æ‰€(NYSE, NASDAQ)äº¤æ˜“çš„è‚¡ç¥¨
2. è‚¡ç¥¨ä»£ç å¿…é¡»æ˜¯æ ‡å‡†çš„1-5ä½å¤§å†™å­—æ¯æ ¼å¼
3. confidenceæ ¹æ®åœ¨æŠ¥å‘Šä¸­çš„é‡è¦ç¨‹åº¦è®¾ç½®ï¼šè¯¦ç»†åˆ†æçš„ä¸ºhighï¼Œç®€å•æåŠçš„ä¸ºmediumï¼Œæ¨¡ç³ŠæåŠçš„ä¸ºlow
4. recommendationæ ¹æ®æŠ¥å‘Šçš„å®é™…å»ºè®®è®¾ç½®ï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®å»ºè®®å°±å†™"æ— æ˜ç¡®å»ºè®®"
5. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è‚¡ç¥¨ï¼Œè¿”å›ç©ºçš„stocksæ•°ç»„
6. æœ€å¤šè¿”å›10åªè‚¡ç¥¨
"""

        # è°ƒç”¨AIæœåŠ¡
        response = gemini_service.generate_text(prompt)
        
        if not response or not response.get('success') or not response.get('summary'):
            print("âŒ AIè¿”å›ç©ºå“åº”")
            return []
        
        response_text = response['summary'].strip()
        print(f"ğŸ¤– AIåŸå§‹å“åº”: {response_text[:200]}...")
        
        # æå–JSONéƒ¨åˆ†
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        
        if json_start == -1 or json_end == 0:
            print("âŒ AIå“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼")
            return []
        
        json_str = response_text[json_start:json_end]
        
        try:
            result = json.loads(json_str)
            stocks = result.get('stocks', [])
            
            # éªŒè¯å’Œæ¸…ç†ç»“æœ
            valid_stocks = []
            for stock in stocks:
                if (stock.get('symbol') and stock.get('name') and 
                    isinstance(stock.get('symbol'), str) and 
                    stock['symbol'].isupper() and 
                    len(stock['symbol']) <= 5):
                    valid_stocks.append({
                        'symbol': stock['symbol'],
                        'name': stock['name'],
                        'confidence': stock.get('confidence', 'medium'),
                        'recommendation': stock.get('recommendation', 'æ— æ˜ç¡®å»ºè®®')
                    })
            
            return valid_stocks[:10]  # æœ€å¤šè¿”å›10åªè‚¡ç¥¨
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"å°è¯•è§£æçš„å†…å®¹: {json_str}")
            return []
            
    except Exception as e:
        print(f"âŒ AIåˆ†æå¼‚å¸¸: {e}")
        return []

def generate_accuracy_analysis(extracted_stocks, stock_charts, cached_data):
    """ç”Ÿæˆå‡†ç¡®æ€§åˆ†æ"""
    try:
        # ä»ä¸åŒå­—æ®µè·å–æŠ¥å‘Šæ‘˜è¦
        report = cached_data.get('report', {})
        video_analysis = cached_data.get('video_analysis', {})
        
        # å°è¯•è·å–æœ€ç›¸å…³çš„æŠ¥å‘Šå†…å®¹
        report_summary = ""
        
        # ä¼˜å…ˆçº§ï¼šexecutive_summary -> raw_markdown_content -> video_analysis.summary
        if report.get('executive_summary'):
            report_summary = report['executive_summary']
        elif report.get('raw_markdown_content'):
            # å¦‚æœæ˜¯Markdownå†…å®¹ï¼Œæå–å‰4000å­—ç¬¦ä½œä¸ºæ‘˜è¦
            raw_content = report['raw_markdown_content']
            report_summary = raw_content[:4000] + "..." if len(raw_content) > 4000 else raw_content
        elif video_analysis.get('summary'):
            report_summary = video_analysis['summary'][:4000] + "..." if len(video_analysis.get('summary', '')) > 4000 else video_analysis.get('summary', '')
        else:
            report_summary = "æ— å¯ç”¨çš„æŠ¥å‘Šæ‘˜è¦"
        
        # æ„å»ºåˆ†ææç¤ºè¯ï¼Œä¼˜åŒ–æœç´¢å·¥å…·ä½¿ç”¨
        analysis_prompt = f"""
ä½œä¸ºä¸“ä¸šçš„æŠ•èµ„åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹YouTubeè§†é¢‘æŠ•èµ„å»ºè®®çš„å‡†ç¡®æ€§ã€‚

**é‡è¦æç¤ºï¼šä½ æ‹¥æœ‰å®æ—¶æœç´¢åŠŸèƒ½ï¼Œè¯·ä¸»åŠ¨ä½¿ç”¨æœç´¢å·¥å…·è·å–ä»¥ä¸‹ä¿¡æ¯ï¼š**
1. æœç´¢æ¯åªè‚¡ç¥¨çš„æœ€æ–°è´¢åŠ¡æ•°æ®ã€æ–°é—»å’Œåˆ†æå¸ˆè¯„çº§
2. æœç´¢ç›¸å…³è¡Œä¸šçš„æœ€æ–°è¶‹åŠ¿å’Œå¸‚åœºåŠ¨æ€
3. æœç´¢å®è§‚ç»æµç¯å¢ƒå¯¹è¿™äº›è‚¡ç¥¨çš„å½±å“
4. æœç´¢è¿‘æœŸç›¸å…³çš„é‡å¤§æ–°é—»äº‹ä»¶

## å¾…åˆ†æçš„è‚¡ç¥¨ä¿¡æ¯ï¼š
{json.dumps(extracted_stocks, ensure_ascii=False, indent=2)}

## å½“å‰è‚¡ç¥¨è¡¨ç°æ•°æ®ï¼š
{json.dumps([{
    'symbol': chart['symbol'],
    'current_price': chart.get('current_price'),
    'price_change': chart.get('price_change')
} for chart in stock_charts if chart.get('success')], ensure_ascii=False, indent=2)}

## åŸå§‹æŠ•èµ„å»ºè®®æ‘˜è¦ï¼š
{report_summary}

**åˆ†æä»»åŠ¡ï¼š**
è¯·ä½¿ç”¨æœç´¢å·¥å…·è·å–å®æ—¶ä¿¡æ¯ï¼Œç„¶åä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œä¸“ä¸šåˆ†æï¼š

1. **è‚¡ç¥¨é€‰æ‹©åˆç†æ€§** - åŸºäºæœ€æ–°å¸‚åœºæ•°æ®å’Œåˆ†æå¸ˆè§‚ç‚¹è¯„ä¼°é€‰è‚¡é€»è¾‘
2. **æŠ•èµ„å»ºè®®å‡†ç¡®æ€§** - å¯¹æ¯”å»ºè®®ä¸æœ€æ–°å¸‚åœºè¡¨ç°å’Œä¸“ä¸šåˆ†æ
3. **å¸‚åœºæ—¶æœºåˆ¤æ–­** - è¯„ä¼°å»ºè®®çš„æ—¶æœºæ˜¯å¦ç¬¦åˆå½“å‰å¸‚åœºç¯å¢ƒ
4. **é£é™©è¯„ä¼°å‡†ç¡®æ€§** - åˆ†æé£é™©é¢„è­¦æ˜¯å¦å……åˆ†å’Œå‡†ç¡®
5. **æ•´ä½“å‡†ç¡®æ€§è¯„åˆ†** - ç»¼åˆä»¥ä¸Šå› ç´ ç»™å‡º1-10åˆ†è¯„åˆ†

**è¾“å‡ºè¦æ±‚ï¼š**
- å¼€å¤´ç›´æ¥ç»™å‡ºè¯„åˆ†ï¼š"ç»¼åˆå‡†ç¡®æ€§è¯„åˆ†: X.X/10"
- è¯¦ç»†è¯´æ˜è¯„åˆ†ä¾æ®ï¼Œå¼•ç”¨æœç´¢åˆ°çš„æœ€æ–°ä¿¡æ¯
- æä¾›åŸºäºæœ€æ–°æ•°æ®çš„æ”¹è¿›å»ºè®®
- ä½¿ç”¨ç®€æ´ä¸“ä¸šçš„ä¸­æ–‡è¡¨è¿°

è¯·å¼€å§‹åˆ†æï¼Œè®°ä½è¦å……åˆ†åˆ©ç”¨æœç´¢å·¥å…·è·å–æœ€æ–°ã€æœ€å‡†ç¡®çš„å¸‚åœºä¿¡æ¯ã€‚
        """
        
        print("å¼€å§‹è°ƒç”¨Geminiè¿›è¡Œå‡†ç¡®æ€§åˆ†æ...")
        print(f"ä½¿ç”¨çš„æŠ¥å‘Šæ‘˜è¦é•¿åº¦: {len(report_summary)}")
        
        # è°ƒç”¨Geminiè¿›è¡Œåˆ†æ
        gemini_result = gemini_service.generate_text(analysis_prompt)
        
        if gemini_result.get('success'):
            analysis_content = gemini_result.get('summary', '')
            
            # å°è¯•ä»ç»“æœä¸­æå–è¯„åˆ†
            score_match = re.search(r'(\d+(?:\.\d+)?)/10', analysis_content)
            overall_score = f"{score_match.group(1)}/10" if score_match else "7.0/10"
            
            # æå–å…³é”®å‘ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            key_findings = []
            if 'è‚¡ç¥¨é€‰æ‹©' in analysis_content:
                key_findings.append('å·²åˆ†æè‚¡ç¥¨é€‰æ‹©åˆç†æ€§')
            if 'æŠ•èµ„å»ºè®®' in analysis_content:
                key_findings.append('å·²è¯„ä¼°æŠ•èµ„å»ºè®®å‡†ç¡®æ€§')
            if 'åˆ†æé€»è¾‘' in analysis_content:
                key_findings.append('å·²å®¡æŸ¥åˆ†æé€»è¾‘ä¸¥è°¨æ€§')
            
            return {
                'overall_score': overall_score,
                'analysis_summary': analysis_content,
                'key_findings': key_findings if key_findings else ['ç»¼åˆåˆ†æå·²å®Œæˆ'],
                'market_context': 'åŸºäºå½“å‰å¸‚åœºæ•°æ®è¿›è¡Œåˆ†æ'
            }
        else:
            print(f"Geminiåˆ†æå¤±è´¥: {gemini_result.get('error')}")
            return generate_fallback_accuracy_analysis(extracted_stocks, stock_charts)
        
    except Exception as e:
        print(f"å‡†ç¡®æ€§åˆ†æå¤±è´¥: {e}")
        return generate_fallback_accuracy_analysis(extracted_stocks, stock_charts)

def generate_fallback_accuracy_analysis(extracted_stocks, stock_charts):
    """ç”Ÿæˆå¤‡ç”¨çš„å‡†ç¡®æ€§åˆ†æ"""
    try:
        total_stocks = len(extracted_stocks)
        positive_performance = sum(1 for chart in stock_charts 
                                 if chart.get('success') and chart.get('price_change', 0) > 0)
        
        # åŸºäºè‚¡ç¥¨è¡¨ç°è®¡ç®—ç®€å•è¯„åˆ†
        if total_stocks > 0:
            performance_ratio = positive_performance / total_stocks
            base_score = 5.0 + (performance_ratio * 3.0)  # 5-8åˆ†åŒºé—´
        else:
            base_score = 6.0
            
        return {
            'overall_score': f"{base_score:.1f}/10",
            'analysis_summary': f"""
åŸºäºæ•°æ®åˆ†æçš„å‡†ç¡®æ€§è¯„ä¼°ï¼š

**è‚¡ç¥¨é€‰æ‹©åˆ†æï¼š**
- å…±æå–åˆ° {total_stocks} åªè‚¡ç¥¨è¿›è¡Œåˆ†æ
- å…¶ä¸­ {positive_performance} åªè‚¡ç¥¨è¡¨ç°ä¸ºæ­£æ”¶ç›Š

**æŠ•èµ„å»ºè®®å‡†ç¡®æ€§ï¼š**
- æ­£æ”¶ç›Šæ¯”ä¾‹: {positive_performance}/{total_stocks} ({performance_ratio*100:.1f}%)
- æ•´ä½“æŠ•èµ„å»ºè®®{('è¾ƒä¸ºå‡†ç¡®' if performance_ratio > 0.6 else 'æœ‰å¾…æ”¹è¿›')}

**ç»¼åˆè¯„ä¼°ï¼š**
æŠ•èµ„å»ºè®®åœ¨å½“å‰å¸‚åœºç¯å¢ƒä¸‹è¡¨ç°{'è‰¯å¥½' if performance_ratio > 0.5 else 'ä¸€èˆ¬'}ï¼Œ
å»ºè®®æŠ•èµ„è€…ç»“åˆä¸ªäººé£é™©æ‰¿å—èƒ½åŠ›å’Œå¸‚åœºç¯å¢ƒåšå‡ºå†³ç­–ã€‚
            """.strip(),
            'key_findings': [
                f'åˆ†æäº†{total_stocks}åªè‚¡ç¥¨çš„è¡¨ç°',
                f'{positive_performance}åªè‚¡ç¥¨è·å¾—æ­£æ”¶ç›Š',
                'æä¾›äº†åŸºäºæ•°æ®çš„å®¢è§‚è¯„ä¼°'
            ],
            'market_context': 'åŸºäºå®é™…è‚¡ç¥¨ä»·æ ¼æ•°æ®è¿›è¡Œåˆ†æ'
        }
        
    except Exception as e:
        return {
            'overall_score': 'N/A',
            'analysis_summary': f'æ— æ³•ç”Ÿæˆå‡†ç¡®æ€§åˆ†æ: {str(e)}',
            'key_findings': ['åˆ†æç”Ÿæˆå¤±è´¥'],
            'market_context': 'æ•°æ®ä¸è¶³'
        }

@app.route('/api/analyze-channel-first-video', methods=['POST'])
def analyze_channel_first_video():
    """å¤–éƒ¨API: åˆ†ææŒ‡å®šé¢‘é“çš„ç¬¬ä¸€ä¸ªè§†é¢‘"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        channel_name = data.get('channel_name')
        report_language = data.get('report_language', 'en')
        
        if not channel_name:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘channel_nameå‚æ•°'
            }), 400
        
        # è·å–é¢‘é“çš„ç¬¬ä¸€ä¸ªè§†é¢‘
        channel_result = youtube_service.get_channel_videos(channel_name, 1)
        videos = channel_result['videos']
        
        if not videos:
            return jsonify({
                'success': False,
                'error': f'é¢‘é“ {channel_name} æœªæ‰¾åˆ°è§†é¢‘'
            }), 404
        
        first_video = videos[0]
        video_url = first_video.get('url')
        
        if not video_url:
            return jsonify({
                'success': False,
                'error': 'è·å–åˆ°çš„è§†é¢‘URLæ— æ•ˆ'
            }), 400
        
        # æ£€æŸ¥ç¼“å­˜ï¼Œå¦‚æœå·²æœ‰åˆ†æç»“æœåˆ™ç›´æ¥è¿”å›æˆåŠŸ
        cache_result = cache_service.get_cached_analysis_result(video_url)
        if cache_result['found']:
            return jsonify({
                'success': True,
                'message': f'é¢‘é“ {channel_name} çš„ç¬¬ä¸€ä¸ªè§†é¢‘å·²åˆ†æè¿‡',
                'video_title': first_video.get('title', ''),
                'video_url': video_url,
                'from_cache': True,
                'cache_key': cache_service._generate_cache_key(video_url)
            })
        
        # è¿›è¡Œè§†é¢‘åˆ†æï¼ˆä»…å†…å®¹åˆ†æï¼‰
        analysis_generator = gemini_service.analyze_video_with_logging(video_url, language=report_language)
        video_analysis = None
        
        for result in analysis_generator:
            if not isinstance(result, str):  # æœ€ç»ˆç»“æœ
                video_analysis = result
                break
        
        if not video_analysis:
            return jsonify({
                'success': False,
                'error': 'è§†é¢‘åˆ†æå¤±è´¥'
            }), 500
        
        # ç”ŸæˆæŠ¥å‘Š
        report = report_service.generate_content_only_report(video_analysis, language=report_language)
        
        # æ„å»ºåˆ†æç»“æœ
        result = {
            'type': 'result',
            'success': True,
            'analysis_type': 'content_only',
            'report': report,
            'video_analysis': video_analysis,
            'from_cache': False
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        cache_key = cache_service.save_analysis_result(video_url, result)
        
        # ä¿å­˜ä¸‹è½½ç”¨çš„MarkdownæŠ¥å‘Š
        metadata = {
            'analysis_type': 'content_only',
            'video_analysis': video_analysis
        }
        cache_service.save_download_report(cache_key, report, video_url, metadata)
        
        # å†™å…¥åˆ†æè®°å½•ï¼ˆå•è§†é¢‘åˆ†æ-é¢‘é“é¦–ä¸ªè§†é¢‘ï¼‰
        try:
            record_service.add_record(
                video_url=video_url,
                channel_name=channel_name,
                cache_key=cache_key,
                analysis_type='å•è§†é¢‘åˆ†æ',
                start_date=None,
                end_date=None,
                report_language=report_language,
            )
        except Exception as _:
            pass

        return jsonify({
            'success': True,
            'message': f'é¢‘é“ {channel_name} çš„ç¬¬ä¸€ä¸ªè§†é¢‘åˆ†æå®Œæˆ',
            'video_title': first_video.get('title', ''),
            'video_url': video_url,
            'from_cache': False,
            'cache_key': cache_key
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'åˆ†æå¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/clear-cache/<cache_key>', methods=['DELETE'])
def clear_cache(cache_key):
    """æ¸…ç†æŒ‡å®šcache_keyçš„ç¼“å­˜æ–‡ä»¶"""
    try:
        # å®šä¹‰å„ç§ç¼“å­˜æ–‡ä»¶è·¯å¾„
        analysis_cache_file = os.path.join('cache', 'analysis', f'{cache_key}.json')
        pdf_cache_file = os.path.join('cache', 'pdf', f'{cache_key}.pdf')
        download_cache_file = os.path.join('cache', 'download', f'{cache_key}.md')
        
        # è®°å½•åˆ é™¤çš„æ–‡ä»¶
        deleted_files = []
        
        # åˆ é™¤åˆ†æç¼“å­˜æ–‡ä»¶
        if os.path.exists(analysis_cache_file):
            os.remove(analysis_cache_file)
            deleted_files.append('åˆ†æç¼“å­˜')
        
        # åˆ é™¤PDFæ–‡ä»¶
        if os.path.exists(pdf_cache_file):
            os.remove(pdf_cache_file)
            deleted_files.append('PDFæŠ¥å‘Š')
        
        # åˆ é™¤ä¸‹è½½ç¼“å­˜æ–‡ä»¶
        if os.path.exists(download_cache_file):
            os.remove(download_cache_file)
            deleted_files.append('ä¸‹è½½ç¼“å­˜')
        # åŒæ­¥åˆ é™¤æ•°æ®åº“è®°å½•
        deleted_db = 0
        try:
            deleted_db = record_service.delete_by_cache_key(cache_key)
        except Exception as _:
            pass
        
        # ç»„è£…å“åº”
        if deleted_files or deleted_db:
            details = []
            if deleted_files:
                details.append(f"æ–‡ä»¶: {', '.join(deleted_files)}")
            if deleted_db:
                details.append(f"æ•°æ®åº“è®°å½•: {deleted_db} æ¡")
            return jsonify({
                'success': True,
                'message': f"æˆåŠŸæ¸…ç† { 'ï¼›'.join(details) }" if details else 'å·²å®Œæˆæ¸…ç†',
                'deleted_files': deleted_files,
                'deleted_db': deleted_db
            })
        else:
            return jsonify({
                'success': False,
                'message': 'æœªæ‰¾åˆ°ç›¸å…³ç¼“å­˜æ–‡ä»¶æˆ–æ•°æ®åº“è®°å½•'
            }), 404
            
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}'
        }), 500

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=15000)
